{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a693c4",
   "metadata": {},
   "source": [
    "# Data Cleanup Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from threading import Lock\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "input_source = \"../artifacts/dataset\"\n",
    "output_source = \"../artifacts/dataset_cleanup\"\n",
    "\n",
    "input_files = [\n",
    "    \"../artifacts/dataset/data.csv\",\n",
    "    \"../artifacts/dataset/phisthank_verified_online.csv\",\n",
    "    \"../artifacts/dataset/url_haus.csv\"\n",
    "]\n",
    "output_path = \"../artifacts/dataset_cleanup/data_cleaned.csv\"\n",
    "\n",
    "cols_req = [\"url\"]\n",
    "cols_additional = [\"type\", \"isMalicious\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53042704",
   "metadata": {},
   "source": [
    "### Define url is safe or not \n",
    "\n",
    "**Reference**\n",
    "\n",
    "- [Cyber crime info center - records repository](https://www.cybercrimeinfocenter.org/records-repository)\n",
    "- [Threat hunting suspicious tlds](https://detect.fyi/threat-hunting-suspicious-tlds-a742c2adbf58)\n",
    "- [Malicious domain extensions](https://www.gomyitguy.com/blog-news-updates/malicious-domain-extensions)\n",
    "- [Url haus](https://urlhaus.abuse.ch/api/#csv)\n",
    "- [Phishtank](https://phishtank.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5614b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MALICIOUS_CONTAIN = [\"defacement\", \"phishing\", \"malware\"]\n",
    "MALICIOUS_TLD = [\n",
    "    '.exe', '.bat', '.cmd', '.scr', '.pif', '.vbs', '.js', '.dll', '.sys', '.drv',\n",
    "    '.hint', '.tmp', '.dat', '.bin', '.msi', '.torrent', '.lnk', '.reg', '.sh', '.flac'\n",
    "]\n",
    "\n",
    "COLUMN_MAPPING = {\n",
    "    'URL': 'url',\n",
    "    'Url': 'url',\n",
    "    'Type': 'type',\n",
    "    'category': 'type',\n",
    "    'Category': 'type',\n",
    "    'label': 'type',\n",
    "    'Label': 'type',\n",
    "    'malicious': 'isMalicious',\n",
    "    'Malicious': 'isMalicious',\n",
    "    'is_malicious': 'isMalicious'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4fa04",
   "metadata": {},
   "source": [
    "### Cleanup Dataset Functions\n",
    "\n",
    "**Requirement column**\n",
    "- `url`\n",
    "\n",
    "**Additional column**\n",
    "- `type`: type of url\n",
    "- `isMalicious`: flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            return {\n",
    "                'success': True,\n",
    "                'data': df,\n",
    "                'file': file_path,\n",
    "                'rows': len(df),\n",
    "                'columns': list(df.columns)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'file': file_path,\n",
    "                'data': None\n",
    "            }\n",
    "    else:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'File not found',\n",
    "            'file': file_path,\n",
    "            'data': None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0494a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_files(input_files, max_workers=4):\n",
    "    print(\"Loading files using thread workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_file = {executor.submit(load_single_file, file_path): file_path \n",
    "                        for file_path in input_files}\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            result = future.result()\n",
    "            \n",
    "            if result['success']:\n",
    "                print(f\"✓ Loaded {result['file']}: {result['rows']} rows, columns: {result['columns']}\")\n",
    "                all_dataframes.append(result['data'])\n",
    "            else:\n",
    "                print(f\"✗ Error loading {result['file']}: {result['error']}\")\n",
    "    \n",
    "    if all_dataframes:\n",
    "        merged_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Merged data: {len(merged_data)} total rows in {elapsed_time:.2f}s\")\n",
    "        return merged_data\n",
    "    else:\n",
    "        print(\"No files were loaded successfully\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df):\n",
    "    for old_name, new_name in COLUMN_MAPPING.items():\n",
    "        if old_name in df.columns:\n",
    "            df.rename(columns={old_name: new_name}, inplace=True)\n",
    "    \n",
    "    if 'url' not in df.columns:\n",
    "        print(\"Error: No 'url' column found in the data\")\n",
    "        return None\n",
    "    \n",
    "    if 'type' not in df.columns:\n",
    "        df['type'] = 'unknown'\n",
    "    \n",
    "    if 'isMalicious' not in df.columns:\n",
    "        df['isMalicious'] = False\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77194aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_urls(df):\n",
    "    initial_count = len(df)\n",
    "    df = df.dropna(subset=['url'])\n",
    "    df = df[df['url'].str.strip() != '']\n",
    "    \n",
    "    print(f\"Removed {initial_count - len(df)} rows with empty URLs\")\n",
    "    \n",
    "    df['url'] = df['url'].str.strip()\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "    print(f\"Removed {initial_count - len(df)} duplicate URLs\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae77275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tld(url):\n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "        \n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc.lower()\n",
    "        \n",
    "        if '.' in domain:\n",
    "            return '.' + domain.split('.')[-1]\n",
    "        return ''\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_malicious_row(row):\n",
    "    url = str(row['url']).lower()\n",
    "    url_type = str(row['type']).lower()\n",
    "    \n",
    "    type_is_malicious = any(malicious_word in url_type for malicious_word in MALICIOUS_CONTAIN)\n",
    "    tld_is_malicious = any(url.endswith(tld.lower()) for tld in MALICIOUS_TLD)\n",
    "    \n",
    "    return type_is_malicious or tld_is_malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_single_batch(batch_data, batch_idx, total_batches, lock, processed_count):\n",
    "    batch_data['isMalicious'] = batch_data.apply(is_malicious_row, axis=1)\n",
    "    \n",
    "    mask_unknown = batch_data['type'] == 'unknown'\n",
    "    batch_data.loc[mask_unknown & batch_data['isMalicious'], 'type'] = 'suspicious'\n",
    "    batch_data.loc[mask_unknown & ~batch_data['isMalicious'], 'type'] = 'benign'\n",
    "    \n",
    "    with lock:\n",
    "        processed_count[0] += len(batch_data)\n",
    "        if batch_idx % 10 == 0 or batch_idx == total_batches - 1:\n",
    "            print(f\"Processed batch {batch_idx + 1}/{total_batches} \"\n",
    "                  f\"({processed_count[0]} URLs classified)\")\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979e664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_urls_threaded(df, max_workers=4, batch_size=1000):\n",
    "    print(f\"Classifying URLs using {max_workers} thread workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    batches = []\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = df.iloc[i:i + batch_size].copy()\n",
    "        batches.append(batch)\n",
    "    \n",
    "    total_batches = len(batches)\n",
    "    print(f\"Processing {total_rows} URLs in {total_batches} batches of {batch_size}\")\n",
    "    \n",
    "    processed_count = [0]\n",
    "    lock = Lock()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_batch = {\n",
    "            executor.submit(classify_single_batch, batch, idx, total_batches, lock, processed_count): idx \n",
    "            for idx, batch in enumerate(batches)\n",
    "        }\n",
    "        \n",
    "        batch_results = {}\n",
    "        for future in as_completed(future_to_batch):\n",
    "            batch_idx = future_to_batch[future]\n",
    "            batch_results[batch_idx] = future.result()\n",
    "    \n",
    "    processed_batches = [batch_results[i] for i in range(total_batches)]\n",
    "    result_df = pd.concat(processed_batches, ignore_index=True)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"URL classification completed in {elapsed_time:.2f}s\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_batch_for_stats(batch_data, tld_list):\n",
    "    stats = {\n",
    "        'total_count': len(batch_data),\n",
    "        'malicious_count': batch_data['isMalicious'].sum(),\n",
    "        'type_counts': batch_data['type'].value_counts().to_dict(),\n",
    "        'tld_counts': {}\n",
    "    }\n",
    "    \n",
    "    for tld in tld_list:\n",
    "        count = batch_data['url'].str.lower().str.endswith(tld.lower()).sum()\n",
    "        if count > 0:\n",
    "            stats['tld_counts'][tld] = count\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics_threaded(df, max_workers=4):\n",
    "    print(\"Generating statistics using thread workers...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    batch_size = 5000\n",
    "    total_rows = len(df)\n",
    "    batches = []\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = df.iloc[i:i + batch_size]\n",
    "        batches.append(batch)\n",
    "    \n",
    "    all_stats = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        analyze_func = partial(analyze_batch_for_stats, tld_list=MALICIOUS_TLD)\n",
    "        futures = [executor.submit(analyze_func, batch) for batch in batches]\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            all_stats.append(future.result())\n",
    "    \n",
    "    total_urls = sum(stat['total_count'] for stat in all_stats)\n",
    "    malicious_count = sum(stat['malicious_count'] for stat in all_stats)\n",
    "    benign_count = total_urls - malicious_count\n",
    "    \n",
    "    combined_type_counts = {}\n",
    "    for stat in all_stats:\n",
    "        for url_type, count in stat['type_counts'].items():\n",
    "            combined_type_counts[url_type] = combined_type_counts.get(url_type, 0) + count\n",
    "    \n",
    "    combined_tld_counts = {}\n",
    "    for stat in all_stats:\n",
    "        for tld, count in stat['tld_counts'].items():\n",
    "            combined_tld_counts[tld] = combined_tld_counts.get(tld, 0) + count\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n=== CLEANUP STATISTICS (Generated in {elapsed_time:.2f}s) ===\")\n",
    "    print(f\"Total URLs: {total_urls}\")\n",
    "    print(f\"Malicious URLs: {malicious_count} ({malicious_count/total_urls*100:.2f}%)\")\n",
    "    print(f\"Benign URLs: {benign_count} ({benign_count/total_urls*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n=== TYPE DISTRIBUTION ===\")\n",
    "    sorted_types = sorted(combined_type_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for url_type, count in sorted_types:\n",
    "        print(f\"{url_type}: {count} ({count/total_urls*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n=== MALICIOUS TLD DETECTION ===\")\n",
    "    if combined_tld_counts:\n",
    "        sorted_tlds = sorted(combined_tld_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        for tld, count in sorted_tlds:\n",
    "            print(f\"URLs ending with {tld}: {count}\")\n",
    "    else:\n",
    "        print(\"No URLs found with malicious TLDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_data(df, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    final_columns = [\"url\", \"type\", \"isMalicious\"]\n",
    "    cleaned_data = df[final_columns].copy()\n",
    "    \n",
    "    cleaned_data.to_csv(output_path, index=False)\n",
    "    print(f\"\\nCleaned data saved to: {output_path}\")\n",
    "    \n",
    "    print(\"\\n=== SAMPLE CLEANED DATA ===\")\n",
    "    print(cleaned_data.head(10))\n",
    "    \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working_section",
   "metadata": {},
   "source": [
    "### Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main_execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cleanup_pipeline(input_files, output_path, max_workers=4):\n",
    "    print(f\"=== STARTING URL DATA CLEANUP PIPELINE (Using {max_workers} workers) ===\\n\")\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    merged_data = load_and_merge_files(input_files, max_workers)\n",
    "    if merged_data is None:\n",
    "        return None\n",
    "    \n",
    "    standardized_data = standardize_columns(merged_data)\n",
    "    if standardized_data is None:\n",
    "        return None\n",
    "    \n",
    "    cleaned_data = clean_urls(standardized_data)\n",
    "    classified_data = classify_urls_threaded(cleaned_data, max_workers)\n",
    "    generate_statistics_threaded(classified_data, max_workers)\n",
    "    final_data = save_cleaned_data(classified_data, output_path)\n",
    "    \n",
    "    total_time = time.time() - pipeline_start\n",
    "    print(f\"\\n=== CLEANUP PIPELINE COMPLETED in {total_time:.2f}s ===\")\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cleaned_df = run_cleanup_pipeline(input_files, output_path, max_workers=4)\n",
    "    \n",
    "    if cleaned_df is not None:\n",
    "        print(f\"\\nFinal cleaned dataset shape: {cleaned_df.shape}\")\n",
    "        print(\"Columns:\", list(cleaned_df.columns))\n",
    "        print(f\"Total records: {len(cleaned_df)}\")\n",
    "        print(f\"Malicious URLs: {cleaned_df['isMalicious'].sum()}\")\n",
    "        print(f\"Benign URLs: {(~cleaned_df['isMalicious']).sum()}\")\n",
    "    else:\n",
    "        print(\"Pipeline failed to complete successfully\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
